<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="XwhPCxJ23A4vdp_xBFDnsoJelhGwCPELOda48Hw_GLQ"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Eigenvalue problem with Physics-informed Neural Network | Geowanderer | Jingtao Min Homepage</title> <meta name="author" content="Jingtao Min"> <meta name="description" content="Solve eigenvalue-eigenfunction pair with PINNs"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://gentlemin.github.io/notes/eigenvalue_pinn/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Geowanderer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">Notes</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repos</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/teaching/">Teaching</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Eigenvalue problem with Physics-informed Neural Network</h1> <p class="post-meta">April 5, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/computation"> <i class="fas fa-tag fa-sm"></i> Computation</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <hr> <h2 id="eigenvalue-problem">Eigenvalue problem</h2> <p>We consider the eigenvalue problem of the general form</p> \[\mathcal{L} u = \lambda ru\] <p>where \(\mathcal{L}\) is a given general differential operator, \(r\) is a given weight function. The unknown variables in this problem are the eigenvalue \(\lambda\), and the corresponding eigenfunction \(u\).</p> <p>PDEs (sometimes ODEs) are always coupled with necessary boundary conditions, whether numerical ones (Dirichlet, Neumann, Robin), or what Boyd calls “natural” ones (function being periodic, or regular or finite at boundaries, etc.). For eigenvalue problems, homogeneous numerical boundary conditions are numerically “consistent” boundary conditions, in that these BCs really lead to matrix eigenvalue problems in FEM or spectral methods. The non-homogeneous BCs seem to be physically well defined, but incur extra vectors in the linear system that cannot be merged to an eigenvalue problem (see also [[Numerics of 1D torsional oscillation]]); thus I am at lost how to implement them. Finally, natural boundary conditions are almost always implicitly satisfied. For instance, periodicity is inherently satisfied by using Fourier basis in spectral methods, and almost all numerical methods only allow finite/regular solutions (it might be possible to choose a set of basis functions with singularities, but I am not sure about the implications).</p> <h3 id="conventional-numerical-solvers-for-eigenvalue-problem">Conventional numerical solvers for eigenvalue problem</h3> <p>Two conventional numerical approaches are discussed in [[Numerics of 1D torsional oscillation]], namely finite-element method (FEM) and spectral method. Both methods use basis expansion to discretize the system (or its weak form, in FEM and Galerkin-spectral method), and the eigenvalue problem of differential operators is reduced to solving a matrix eigenvalue problem.</p> <p><br></p> <hr> <h2 id="physics-informed-neural-network">Physics-informed neural network</h2> <p>Consider an arbitrary differential equation of the form</p> \[\mathcal{L}(u) = 0,\qquad x\in\Omega\] <p>with boundary condition</p> \[F(u)|_{\partial \Omega} = 0.\] <p>Unlike the operator in eigenvalue problem, now the operator \(\mathcal{L}\) here includes all fields, including the forcing terms. It is also not necessarily linear. Another approach to solving such PDEs is to build the surrogate solution via a neural network</p> \[\hat{u}(x) = f(x;\theta)\] <p>where \(\theta\) denotes the collective of neural network parameters. The task is then to train the network, optimize on the parameters, to seek a good approximate \(f(\cdot, \hat{\theta})\) of the true \(u^*(\cdot)\). There are different ways to construct the optimization problem. In particular, physics-informed neural network (PINN, <a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125" rel="external nofollow noopener" target="_blank">Rassi et al. 2019</a>) suggests the objective function be formed by penalizing the combined loss of PDE residual and the boundary discrepancy</p> \[L(\theta) = \lambda_\mathrm{int} \sum_{x_i\in D_\mathrm{int}} L[\mathcal{L}f(x;\theta)|_{x_i}] + \lambda_\mathrm{BC} \sum_{x_i\in D_\mathrm{BC}} L[F(f(x_i;\theta))]\] <p>where \(L[\cdot]\) is point-wise measure of the loss. For instance, one can take the \(\ell^2\) norm, so that \(L[y] = y^2\) for every scalar \(y\). Therefore, except for at the boundary where the values are known, the PDE loss and the objective function does not require any further knowledge of the solution. This is why PINN is generally considered unsupervised learning.</p> <h3 id="applications-and-variants">Applications and variants</h3> <p>The aforementioned framework is just the most simple (however general) framework for PINNs. One of the many merits of PINNs is that the optimization stage can be easily made compatible with inverse problems, and the approximate forward field can be trained together with the underlying model parameters. For further info, see <a href="http://arxiv.org/abs/2201.05624" rel="external nofollow noopener" target="_blank">Cuomo et al., 2022</a>.</p> <p><br></p> <hr> <h2 id="eigenvalue-problem-with-pinns">Eigenvalue problem with PINNs</h2> <p>We return to the eigenvalue problem with the form \(\mathcal{L}u = \lambda r u\) in the beginning. Solving the eigenvalue problem is slightly different from the aforementioned framework, because</p> <ul> <li>In eigenvalue problem, both the eigenvalue and eigenfunction (i.e. the eigenpair) are sought, not just the solution field;</li> <li>Eigenvalue problem typically contains a bunch of eigenpairs, usually even infinitely many (but in most system relevant to physics, numerably infinitely many, so the eigenvalues are quantized). Therefore, the PINN for eigenvalue problem should</li> <li>simultaneously optimize on the eigenvalue and the eigenfunction;</li> <li>can have additional DOF as to which eigenvalue to search for.</li> </ul> <p>For eigenvalue discovery, a typical approach is to build a network alongside an optimizable scalar value as the eigenvalue, and optimize on both.</p> \[L(\theta, \lambda) = \lambda_\mathrm{int} \sum_{x_i\in D_\mathrm{int}} L[\mathcal{L}f(x;\theta)|_{x_i} - \lambda r(x_i)f(x_i;\theta)] + \lambda_\mathrm{BC} \sum_{x_i\in D_\mathrm{BC}} L[F(f(x_i;\theta))]\] <p>This approach is used in <a href="https://ieeexplore.ieee.org/document/9891944" rel="external nofollow noopener" target="_blank">Jin et al. 2022</a>, <a href="https://link.aps.org/doi/10.1103/PhysRevD.106.124047" rel="external nofollow noopener" target="_blank">Cornell et al. 2022</a>. Another, more exotic method, is to use the variational form with Rayleigh-quotient (<a href="https://www.sciencedirect.com/science/article/pii/S1007570421003531" rel="external nofollow noopener" target="_blank">Kovacs et al., 2022</a>).</p> <p>For multiple eigenvalue discovery, <a href="https://ieeexplore.ieee.org/document/9891944" rel="external nofollow noopener" target="_blank">Jin et al. 2022</a> suggests two approaches: either add an additional “driving” term \(e^{-\lambda + c}\), and progressively increase \(c\), to drive the neural network to search for larger eigenvalues, or add an orthogonal condition, so that the new eigenfunction sought should be orthogonal to the pre-existing eigenfunctions. The former now requires extra <em>ad hoc</em> choices, while the latter only works when the eigenfunctions are known to be orthogonal, e.g. for Hermitian operators, and the computational expense for evaluating orthogonality increases for higher eigenvalues.</p> <p>There is one other problem concerning eigenvalue problems - the system always has trivial solutions, i.e. \(u\equiv 0\), arbitrary \(\lambda\). There are also different proposals as to how to avoid the trivial solution. <a href="http://arxiv.org/abs/2010.05075" rel="external nofollow noopener" target="_blank">Jin et al., 2020</a> proposes penalizing on \(1/\lambda^2\) and \(1/u^2\); similarly, <a href="https://ieeexplore.ieee.org/document/9891944" rel="external nofollow noopener" target="_blank">Jin et al. 2022</a> introduces the norm loss. I use the norm loss, which takes the form</p> \[L_\mathrm{norm}(\theta, \lambda) = \left[1 - \left(\frac{1}{|D_\mathrm{int}|}\sum_{x_i\in D_\mathrm{int}} f(x;\theta)^p \right)^{1/p}\right]^2.\] <p>The most natural choice is \(p=2\). This will try to find the eigenfunction that is normalized (in the \(p\)-norm sense) to 1.</p> <h3 id="example-1-standing-wave">Example 1: standing wave</h3> <p>The classical textbook eigenvalue problem is described by the following Sturm-Liouville problem</p> \[\left\{\begin{aligned} &amp;\frac{\partial^2 u}{\partial x^2} = \lambda u,\quad x \in (0, 1) \\ &amp;u|_{x=0,1} = 0 \end{aligned}\right.\] <p>which has eigenvalues and eigenfunctions</p> \[\lambda_k = (k\pi)^2,\quad u_k = \sin k\pi x,\quad k \in \mathbb{N} \quad \Longrightarrow \quad u_k = \sqrt{2} \sin k\pi x.\] <p>The latter \(u\) is the eigenfunction with normalized norm, i.e. \(\int u_k^2 dx = 1\).</p> <h3 id="example-2">Example 2:</h3> <p>The eigenvalue problem is given by the Legendre differential equation</p> \[\mathcal{L}u = \lambda u,\qquad \mathcal{L}u = \frac{d}{dx}\left[(1 - x^2)\frac{du}{dx}\right] = (1 - x^2) \frac{d^2 u}{dx^2} - 2x \frac{du}{dx},\qquad x \in (-1, 1)\] <p>There is no explicit (or as Boyd puts it, “numerical”) boundary condition to this problem; instead, the mere requirement of the solution being regular and finite at endpoints is sufficient to yield quantized eigenvalues.</p> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/epochs.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/epochs.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/epochs.gif-1400.webp"></source> <img src="/assets/img/epochs.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Eigenvalue-eigenfunction searching for Legendre equation, with driver c set to 100. The target solution it converges to is the normalized Legendre polynomial of degree 10, and the eigenvalue -n(n+1)=-110. </div> <p><br></p> <hr> <h2 id="faq">FAQ</h2> <p>Here I summarize the frequently-asked-questions (asked by myself) and the tricks that worked, as well as the intuition behind.</p> <ul> <li> <strong>Stagnating eigenvalue</strong>. I usually initialize \(\lambda = c\). It is often the case when \(c\) is far away from the actual \(\lambda^*\), the eigenvalue will stagnate near \(c\) and refuse to go further. This typically happens in training neural networks because of the strong nonlinearity of the functional space. When the solution is far from optimal, it is necessary for the neural network to have strong enough momentum in order to explore the topography of the loss function. In this case, the gradient w.r.t. \(\lambda\) is usually too small for \(\lambda\) to vary significantly. <ul> <li> <strong><em>Solution</em></strong>: Fine-tune the driver term in the form \(e^{(-\lambda + c)/\alpha}\). \(\alpha\) is a scaling factor, and shows the estimated magnitude of \(\lambda^* - c\). For instance, in the standing wave problem, when searching for \(k=7\) solution, which has an eigenvalue of \(480\), and the \(c\) is set to be \(400\), then if one uses the original form, i.e. \(\alpha = 1\), the driver term already has too small a gradient at \(\lambda = 405\). At this stage \(\lambda\) cannot go further. If however, one estimate the distance to be \(\sim 10\), and use \(\alpha=10\) or \(20\), the method usually works.</li> </ul> </li> <li> <strong>Spurious eigenfunctions</strong>. When searching for high-frequency eigenfunctions, the network sometimes converges to a spurious solution, with several pieces of solution get wielded together. Therefore, the solution satisfies the eigenvalue problem piece-wise, and at the junction forms a narrow spike in its derivatives. <strong>Since the spike is narrow, the contribution to the PDE loss is still small, therefore the training loss may still seem ideal.</strong> In particular, the network can have zero solution on part of the function, wielded with another part of the approximately true solution. This is favorable for the neural network because low-frequency trivial solution is much easier to construct. <ul> <li> <strong><em>Solution</em></strong>: increase number of interior points. As the interior points increase, the network has to go for sharper, narrow spikes to achieve the same training loss, which ultimately outweighs the cost of finding the actual smooth eigenfunction.</li> </ul> </li> <li> <strong>Zero solution</strong>: the symptom is characterized by zero solution or very small amplitude solution, with the magnitude loss \(L_\mathrm{norm}\) platforms early on at \(1\). <ul> <li> <strong><em>Solution</em></strong>: this means the norm penalty just isn’t strong enough. Increase the regularization usually solves the problem.</li> </ul> </li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Jingtao Min. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. <a href="https://hits.sh/gentlemin.github.io_notes/eigenvalue_pinn.md" rel="external nofollow noopener" target="_blank"><img alt="Hits" src="https://hits.sh/gentlemin.github.io_notes/eigenvalue_pinn.md.svg?color=9277f0"></a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{})});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>